# services/arangodb_service.py
# ë³€ê²½ í•µì‹¬:
# - ë¡œì»¬ ë””ìŠ¤í¬ ëŒ€ì‹  ArangoDBì— "íŒŒì¼ ë³¸ë¬¸(content)"ë¥¼ ì €ì¥í•˜ëŠ” repo_files ì»¬ë ‰ì…˜ ì¶”ê°€
# - repos ë©”íƒ€ ì»¬ë ‰ì…˜ ì¶”ê°€
# - code_analysisëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€(íŒŒì‹± ê²°ê³¼ ì €ì¥)
# - ê³µí†µ insert/get ìœ í‹¸ì„ ì•ˆì „í•œ upsert í˜•íƒœë¡œ ê°œì„ (409 ë°©ì§€)
# - STARTS_WITH/ENDSWITH ë¯¸ì§€ì› í™˜ê²½ì„ ìœ„í•´ LIKE/CONCAT ì‚¬ìš©
# - ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ì¡°íšŒ ì„±ëŠ¥ ë³´ê°•
# - âœ… ARANGODB_URL(.env) ìš°ì„  ì ìš©: ngrok/https ë“± ì™¸ë¶€ í„°ë„ URLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©

from datetime import datetime
from typing import List, Optional
import logging
import os

from arango import ArangoClient
from arango.exceptions import (
    AQLQueryExecuteError,
    ArangoServerError,
    DocumentInsertError,
)

from config import (
    ARANGODB_HOST, ARANGODB_PORT,
    ARANGODB_USERNAME, ARANGODB_PASSWORD, ARANGODB_DB
)

# -----------------------------------
# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ARANGODB_URL ìš°ì„ )
# -----------------------------------
ARANGODB_URL = os.getenv("ARANGODB_URL")  # ì˜ˆ: https://xxxx.ngrok-free.app
EFFECTIVE_HOSTS = ARANGODB_URL or f"http://{ARANGODB_HOST}:{ARANGODB_PORT}"

logging.getLogger().setLevel("INFO")
logging.info(
    f"[ARANGO CONF] hosts={EFFECTIVE_HOSTS} user={ARANGODB_USERNAME} db={ARANGODB_DB}"
)

# hosts ì—ëŠ” http://host:port ë˜ëŠ” https://ë„ ê°€ëŠ¥
client = ArangoClient(hosts=EFFECTIVE_HOSTS)
db = client.db(ARANGODB_DB, username=ARANGODB_USERNAME, password=ARANGODB_PASSWORD)


def ensure_collections():
    """í•„ìš” ì»¬ë ‰ì…˜/ì¸ë±ìŠ¤ ë³´ì¥ (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•´ë„ ì•ˆì „)"""
    for name in ["repos", "repo_files", "code_analysis", "mindmap_nodes", "mindmap_edges", "code_recommendations"]:
        if not db.has_collection(name):
            if name.endswith("_edges"):
                db.create_collection(name, edge=True)
            else:
                db.create_collection(name)

    # ì¸ë±ìŠ¤ ìƒì„±(ì¤‘ë³µ ì—ëŸ¬ëŠ” ë¬´ì‹œ)
    try:
        rf = db.collection("repo_files")
        rf.add_hash_index(["repo_id"])
        rf.add_hash_index(["repo_id", "path"])
    except Exception:
        pass

    try:
        ca = db.collection("code_analysis")
        ca.add_hash_index(["repo_id"])
        ca.add_hash_index(["filename"])
    except Exception:
        pass


def insert_document(collection_name: str, data: dict):
    """
    ê³µí†µ insert â†’ ì•ˆì „í•œ upsert.
    - ê°™ì€ _key ì¬ì‚½ì… ì‹œ 409ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì„ í™•ì¸ + overwrite_mode="replace" ì ìš©
    - ë™ì‹œì„±ìœ¼ë¡œ 1210ì´ ë°œìƒí•˜ë©´ updateë¡œ ë§ˆë¬´ë¦¬
    """
    ensure_collections()
    if not db.has_collection(collection_name):
        if collection_name.endswith("_edges") or collection_name == "mindmap_edges":
            db.create_collection(collection_name, edge=True)
        else:
            db.create_collection(collection_name)

    collection = db.collection(collection_name)

    # _keyê°€ ìˆê³  ì´ë¯¸ ì¡´ì¬í•˜ë©´ update ë©±ë“± ì²˜ë¦¬
    if "_key" in data and collection.has(data["_key"]):
        return collection.update(data)

    try:
        # ì¼ë¶€ ë²„ì „ì—ì„  overwrite=Trueë§Œìœ¼ë¡œ replace ë³´ì¥ì´ ì•ˆ ë¨ â†’ overwrite_mode ëª…ì‹œ
        return collection.insert(
            data,
            overwrite=True,
            overwrite_mode="replace",  # í•µì‹¬!
        )
    except DocumentInsertError as e:
        # ë“œë¬¼ê²Œ ê²½í•©ìœ¼ë¡œ unique constraintê°€ ë‹¤ì‹œ ë‚˜ë©´ updateë¡œ ë§ˆë¬´ë¦¬
        if "unique constraint violated" in str(e) and "_key" in data and collection.has(data["_key"]):
            return collection.update(data)
        raise


def document_exists(collection_name: str, key: str) -> bool:
    if not db.has_collection(collection_name):
        return False
    return db.collection(collection_name).has(key)


def get_document_by_key(collection_name: str, key: str):
    if not db.has_collection(collection_name):
        return None
    return db.collection(collection_name).get({"_key": key})


def get_documents_by_repo_url_prefix(collection_name: str, prefix: str):
    aql = f"""
    FOR doc IN {collection_name}
      /* STARTS_WITH(doc.repo_url, @prefix) ëŒ€ì‹  LIKEë¡œ ëŒ€ì²´ */
      FILTER doc.repo_url LIKE CONCAT(@prefix, '%')
      RETURN doc
    """
    return list(db.aql.execute(aql, bind_vars={"prefix": prefix}))


def get_documents_by_key_prefix(collection_name: str, prefix: str):
    aql = f"""
    FOR doc IN {collection_name}
      /* STARTS_WITH(doc._key, @prefix) ëŒ€ì‹  LIKEë¡œ ëŒ€ì²´ */
      FILTER doc._key LIKE CONCAT(@prefix, '%')
      RETURN doc
    """
    return list(db.aql.execute(aql, bind_vars={"prefix": prefix}))


# ---------- ë ˆí¬/íŒŒì¼ ì €ì¥ ì „ìš© ìœ í‹¸ ----------

def path_key(repo_id: str, path: str) -> str:
    """_key ê·œì¹™: repo_id__ê²½ë¡œ(ìŠ¬ë˜ì‹œëŠ” __ë¡œ)"""
    return f"{repo_id}__{path.replace('/', '__')}"


def upsert_repo(repo_id: str, repo_url: str, default_branch: str = "main"):
    """ë ˆí¬ ë©”íƒ€ upsert"""
    ensure_collections()
    coll = db.collection("repos")
    doc = {
        "_key": repo_id,
        "repo_url": repo_url,
        "default_branch": default_branch,
        "fetched_at": datetime.utcnow().isoformat() + "Z",
    }
    if coll.has(repo_id):
        return coll.update(doc)
    return coll.insert(doc)


def upsert_repo_file(repo_id: str, path: str, language: str, content: str, size: int, sha: Optional[str] = None):
    """íŒŒì¼ ë³¸ë¬¸ ì €ì¥ì†Œ. contentê¹Œì§€ DBì— ì €ì¥."""
    ensure_collections()
    coll = db.collection("repo_files")
    key = path_key(repo_id, path)
    doc = {
        "_key": key,
        "repo_id": repo_id,
        "path": path,              # repo ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ
        "language": language,
        "size": size,
        "sha": sha,
        "content": content,        # ğŸ”¸ íŒŒì¼ ë³¸ë¬¸
        "fetched_at": datetime.utcnow().isoformat() + "Z",
    }
    if coll.has(key):
        return coll.update(doc)
    return coll.insert(doc)


def get_repo_file_content(repo_id: str, path: str) -> Optional[str]:
    """repo_filesì—ì„œ ë³¸ë¬¸ ë°”ë¡œ ì¡°íšŒ"""
    ensure_collections()
    key = path_key(repo_id, path)
    if not db.collection("repo_files").has(key):
        return None
    return db.collection("repo_files").get(key).get("content")


def list_repo_files(repo_id: str) -> List[dict]:
    """ë ˆí¬ì˜ íŒŒì¼ ëª©ë¡/ë©”íƒ€ ì¡°íšŒ"""
    ensure_collections()
    aql = """
    FOR f IN repo_files
      FILTER f.repo_id == @repo_id
      RETURN { path: f.path, language: f.language, size: f.size, fetched_at: f.fetched_at }
    """
    return list(db.aql.execute(aql, bind_vars={"repo_id": repo_id}))


def ensure_mindmap_graph_exists():
    if not db.has_graph("mindmap_graph"):
        graph = db.create_graph("mindmap_graph")
        db.create_collection("mindmap_nodes")
        db.create_collection("mindmap_edges", edge=True)
        graph.create_edge_definition(
            edge_collection="mindmap_edges",
            from_vertex_collections=["mindmap_nodes"],
            to_vertex_collections=["mindmap_nodes"]
        )
    else:
        if not db.has_collection("mindmap_nodes"):
            db.create_collection("mindmap_nodes")
        if not db.has_collection("mindmap_edges"):
            db.create_collection("mindmap_edges", edge=True)

def get_repo_url_by_id(map_id: str) -> Optional[str]:
    """repos ì»¬ë ‰ì…˜ì—ì„œ repo_url ì¡°íšŒ"""
    ensure_collections()
    if not db.has_collection("repos"):
        return None
    doc = db.collection("repos").get(map_id)
    return doc.get("repo_url") if doc else None

def delete_mindmap(map_id: str, also_recommendations: bool = True) -> dict:
    """í•´ë‹¹ map_idì˜ ë§ˆì¸ë“œë§µ(ë…¸ë“œ/ì—£ì§€) ì œê±°. ì„ íƒì ìœ¼ë¡œ code_recommendationsë„ ì •ë¦¬."""
    ensure_collections()
    aql = """
    LET e = (FOR x IN mindmap_edges FILTER x.map_id == @map_id REMOVE x IN mindmap_edges RETURN 1)
    LET n = (FOR x IN mindmap_nodes FILTER x.map_id == @map_id REMOVE x IN mindmap_nodes RETURN 1)
    LET r = (
      FOR x IN code_recommendations
        FILTER x.map_id == @map_id
        REMOVE x IN code_recommendations
        RETURN 1
    )
    RETURN {
      edges_removed: LENGTH(e),
      nodes_removed: LENGTH(n),
      recs_removed: LENGTH(r)
    }
    """
    # also_recommendations=falseë©´ recsëŠ” ì„¸ì§€ì§€ ì•Šë„ë¡ ë³„ë„ ë¶„ê¸°
    if not also_recommendations:
        aql = """
        LET e = (FOR x IN mindmap_edges FILTER x.map_id == @map_id REMOVE x IN mindmap_edges RETURN 1)
        LET n = (FOR x IN mindmap_nodes FILTER x.map_id == @map_id REMOVE x IN mindmap_nodes RETURN 1)
        RETURN { edges_removed: LENGTH(e), nodes_removed: LENGTH(n), recs_removed: 0 }
        """
    res = list(db.aql.execute(aql, bind_vars={"map_id": map_id}))
    return res[0] if res else {"edges_removed": 0, "nodes_removed": 0, "recs_removed": 0}

def ensure_collections():
    """í•„ìš” ì»¬ë ‰ì…˜/ì¸ë±ìŠ¤ ë³´ì¥ (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•´ë„ ì•ˆì „)"""
    for name in ["repos", "repo_files", "code_analysis", "mindmap_nodes", "mindmap_edges",
                 "code_recommendations", "mindmap_prompts"]:
        if not db.has_collection(name):
            if name.endswith("_edges"):
                db.create_collection(name, edge=True)
            else:
                db.create_collection(name)

    try:
        rf = db.collection("repo_files")
        rf.add_hash_index(["repo_id"])
        rf.add_hash_index(["repo_id", "path"])
    except Exception: pass

    try:
        ca = db.collection("code_analysis")
        ca.add_hash_index(["repo_id"])
        ca.add_hash_index(["filename"])
    except Exception: pass

    try:
        mp = db.collection("mindmap_prompts")
        mp.add_hash_index(["mindmap_id"])
        mp.add_hash_index(["idempotency_key"])
        mp.add_persistent_index(["created_at"])
    except Exception: pass


# -------------------- í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ --------------------

def insert_prompt_doc(doc: dict) -> str:
    """í”„ë¡¬í”„íŠ¸ ê¸°ë¡ì„ ë‚¨ê¸°ê³  _keyë¥¼ ë°˜í™˜"""
    ensure_collections()
    import hashlib, time
    coll = db.collection("mindmap_prompts")
    idem = (doc.get("idempotency_key") or "")[:48]
    if idem:
        # ê°™ì€ idempotency_keyê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        cursor = db.aql.execute("""
          FOR p IN mindmap_prompts
            FILTER p.idempotency_key == @k
            LIMIT 1
            RETURN p
        """, bind_vars={"k": idem})
        exist = next(iter(cursor), None)
        if exist:
            return exist["_key"]

    raw = f"{doc.get('mindmap_id','')}:{time.time_ns()}".encode()
    key = "p_" + hashlib.md5(raw).hexdigest()[:24]
    to_insert = {
        "_key": key,
        "mindmap_id": doc.get("mindmap_id"),
        "prompt": doc.get("prompt"),
        "mode": doc.get("mode"),
        "target_nodes": doc.get("target_nodes"),
        "related_files": doc.get("related_files"),
        "ai_summary": doc.get("ai_summary"),
        "status": doc.get("status", "SUCCEEDED"),
        "idempotency_key": idem or None,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "updated_at": datetime.utcnow().isoformat() + "Z",
    }
    coll.insert(to_insert)
    return key

def get_prompt_doc(mindmap_id: str, prompt_id: Optional[str]) -> Optional[dict]:
    ensure_collections()
    coll = db.collection("mindmap_prompts")
    if prompt_id:
        if coll.has(prompt_id):
            return coll.get(prompt_id)
        return None
    # ìµœì‹  í”„ë¡¬í”„íŠ¸ 1ê±´
    cursor = db.aql.execute("""
      FOR p IN mindmap_prompts
        FILTER p.mindmap_id == @mid
        SORT DATE_TIMESTAMP(p.created_at) DESC
        LIMIT 1
        RETURN p
    """, bind_vars={"mid": mindmap_id})
    return next(iter(cursor), None)

def upsert_prompt_title(prompt_id: str, title: str, summary: str):
    ensure_collections()
    coll = db.collection("mindmap_prompts")
    if coll.has(prompt_id):
        doc = coll.get(prompt_id)
        doc["title"] = title
        doc["ai_summary"] = summary
        doc["updated_at"] = datetime.utcnow().isoformat() + "Z"
        coll.update(doc)


# -------------------- ê·¸ë˜í”„ ì—…ì„œíŠ¸ (í™•ì¥ìš©) --------------------

def upsert_nodes_edges(map_id: str, nodes: list[dict], edges: list[dict], default_mode: str = "FEATURE") -> list[str]:
    """
    nodes: [{key?, label, meta?}]  key ì—†ìœ¼ë©´ label ê¸°ë°˜ìœ¼ë¡œ ìƒì„± ì¶”ì²œ X â†’ ì—¬ê¸°ì„  í•„ìˆ˜ë¡œ ê°„ì£¼
    edges: [{from, to, type?}]
    ë°˜í™˜: ë³€ê²½/ì‹ ê·œ ë…¸ë“œ í‚¤ ë¦¬ìŠ¤íŠ¸
    """
    ensure_collections()
    changed: list[str] = []

    ncoll = db.collection("mindmap_nodes")
    ecoll = db.collection("mindmap_edges")

    for n in nodes:
        key = n.get("key")
        label = n.get("label") or key
        if not key:
            # í‚¤ê°€ ì—†ë‹¤ë©´ labelì„ í‚¤ë¡œ ì“°ë˜ í•´ì‹œí™” ê¶Œì¥ â€“ ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ í•´ì‹œ
            import hashlib
            raw = f"{map_id}:{default_mode}:{label}".encode("utf-8")
            key = hashlib.md5(raw).hexdigest()[:12]

        doc = {
            "_key": key,
            "map_id": map_id,
            "label": label,
            "related_files": (n.get("meta", {}) or {}).get("files", []),
            "mode": (n.get("meta", {}) or {}).get("mode", default_mode),
            "node_type": (n.get("meta", {}) or {}).get("node_type")
        }
        if ncoll.has(key):
            ncoll.update(doc)
        else:
            ncoll.insert(doc)
        changed.append(key)

    for e in edges:
        fr = e.get("from") or e.get("from_")
        to = e.get("to")
        et = e.get("type", "RELATES_TO")
        if not fr or not to:
            continue
        edge_key = f"{fr}__{to}__{et}".replace("/", "_")
        doc = {
            "_key": edge_key,
            "map_id": map_id,
            "_from": f"mindmap_nodes/{fr}",
            "_to": f"mindmap_nodes/{to}",
            "edge_type": et
        }
        if ecoll.has(edge_key):
            ecoll.update(doc)
        else:
            ecoll.insert(doc)

    return list(set(changed))